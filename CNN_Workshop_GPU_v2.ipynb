{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN) - Workshop\n",
    "\n",
    "## GPU Version: high-res rgb images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules and Dataset\n",
    "What is it gonna be? Cats / Dogs or Zebras / Elephants?\n",
    "Make your choice!\n",
    "* Choose a data set by setting the path\n",
    "* Choose an image size (read comments in code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from keras.preprocessing.image import array_to_img, load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Path to image data\n",
    "path = 'D:/Studium_GD/Zooniverse/Data/transfer_learning_project/images/4715/'\n",
    "#path = '/host/data/cnn_workshop/EleZebra/'\n",
    "#path = '/host/data/cnn_workshop/CatDog/'\n",
    "\n",
    "# get class labels\n",
    "class_labels = os.listdir(path + 'train')\n",
    "\n",
    "# image size for training, should be at least (150, 150, 3)\n",
    "# it doesn't make sense to increase size with respect to original images\n",
    "# the larger the size the more computationally expensive it is to train a CNN\n",
    "# recommended for Cat/Dog: (150, 150, 3)\n",
    "#recommended for Zebra / Elephant: (250, 250, 3)\n",
    "image_size_for_training = (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at your images\n",
    "We choose some random images and display them. It's always good to know what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some images\n",
    "def look_at_imgs(path, n=5):\n",
    "    files = os.listdir(path)\n",
    "    # choose random images\n",
    "    random.seed(23)\n",
    "    random_ids = random.sample(population=range(0, len(files)), k=n)\n",
    "    random_imgs = [files[x] for x in random_ids]\n",
    "\n",
    "    # display\n",
    "    all_imgs = list()\n",
    "    for i in range(0, len(random_ids)):\n",
    "        img = load_img(path + random_imgs[i])\n",
    "        all_imgs.append(img)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print(path + random_imgs[i])\n",
    "    \n",
    "    return all_imgs\n",
    "\n",
    "random_imgs_0 = look_at_imgs(path + 'train/' + class_labels[0] + '/')\n",
    "random_imgs_1 = look_at_imgs(path + 'train/' + class_labels[1] + '/')\n",
    "\n",
    "# collect random images\n",
    "random_imgs = random_imgs_0\n",
    "random_imgs.extend(random_imgs_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators\n",
    "\n",
    "Keras _ImageDataGenerator_ implements transformations, pre-processing and serving of image data during training. For example, streaming image batches from a directory while training a model. The data pre-processing & streaming is done on CPU, while the model training is done on GPU to increase efficiency (GPU version).\n",
    "\n",
    "Data augmentation is a process where images are artificially altered to create 'new'data that is similar to the original data. This helps to increase your data set size & to avoid overfitting. Data augmentation operations are: flipping, cropping & zooming.\n",
    "\n",
    "Data pre-processing operations like standardizations of pixel values have been shown to increase the efficiency of model training. Operations include: featurewise_center & featurewise_std_normalization.\n",
    "\n",
    "https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Generator\n",
    "# pre-processing of images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# batch size, the number of images returned by one call of the data generator\n",
    "# should be a number between 1 and approx. 256 (can be hardware limiting)\n",
    "batch_size = 32\n",
    "\n",
    "# generator for training data - specify pre-processing and transformations here\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# generator for test data (no data augmentation! but the same pre-processing as for the training data)\n",
    "datagen_test = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "# generator to get raw images to estimate transformation parameters like mean and std of pixel values\n",
    "datagen_raw = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    "    )\n",
    "\n",
    "raw_gen = datagen_raw.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=1000,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# get a single batch of raw images (1000 images)\n",
    "X_raw = raw_gen.next()\n",
    "\n",
    "# fit data generators on raw data to calculate transformations\n",
    "# use same batch of training data to fit train/test generators\n",
    "datagen_train.fit(X_raw[0])\n",
    "datagen_test.fit(X_raw[0])\n",
    "\n",
    "# fetch data from directory in specified batch sizes for all the generators\n",
    "train_generator = datagen_train.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "test_generator = datagen_test.flow_from_directory(\n",
    "    path + 'test/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# dummy data fetcher to look at data\n",
    "dummy_gen = datagen_train.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=5,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# lets take a look at what the data generator actually does and returns\n",
    "data_batch = dummy_gen.next()\n",
    "for i in range(0, len(data_batch[1])):\n",
    "    print(\"Label: %s\" % data_batch[1][i])\n",
    "    plt.imshow(array_to_img(data_batch[0][i, :]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not happy with the output, e.g. too crazy data augmentation, feel free to change the _ImageDataGenerator_ and run the cell again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers & Model Architecture\n",
    "Now we define a model architecture, which consists of sequential layers of operations. The 2 main blocks of a CNN are the convolutional part and the fully connected part.\n",
    "\n",
    "Convolutional layers: These layers extract features from the input (e.g. edges, corners, patterns). This block starts directly at the beginning (e.g. functions _Conv2D()_) and typically consists of convolution layers (_Conv2d()_), followed by an activation (nowadays ReLu activation), and a pooling layer (e.g. _MaxPooling2D()_). Multiple such constructs can be stacked.\n",
    "* https://keras.io/layers/convolutional/\n",
    "\n",
    "The fully connected part represents a classical neural network consisting of an input layer (_Flatten()_) which takes the features from the convolutional part, some hidden layer(s) (_Dense()_), and finally an output layer (_Dense()_, with an activation function that results in probabilities for classification tasks, like sigmoid or softmax).\n",
    "* https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize some of the initialized layers applied to an image\n",
    "\n",
    "Change the _Conv2D()_ layers, or add more layers after that and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets initialize a model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, # number of filter layers\n",
    "                (3,3), # x, y dimension of kernel (we're going for a 3x3 kernel)\n",
    "                input_shape=image_size_for_training))\n",
    "# Lets add a new activation layer!\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(5,5)))\n",
    "\n",
    "# predict our random images on a un-trained network\n",
    "#data_batch = dummy_gen.next()\n",
    "conv_out = model.predict(data_batch[0])\n",
    "\n",
    "# here we get rid of that added dimension and plot the image\n",
    "def visualize_img(model, img):\n",
    "    # Keras expects batches of images, so we have to add a dimension to trick it into being nice\n",
    "    img_batch = np.expand_dims(img,axis=0)\n",
    "    conv_img = model.predict(img_batch)\n",
    "    conv_img = np.squeeze(conv_img, axis=0)\n",
    "    print(conv_img.shape)\n",
    "    plt.imshow(array_to_img(conv_img))\n",
    "    plt.show()\n",
    "\n",
    "visualize_img(model, data_batch[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sequential model (layer based)\n",
    "model = Sequential()\n",
    "# Convolutional layer over 2 dimensions\n",
    "# 32 filters, each with a size of 3x3 pixels\n",
    "model.add(Conv2D(32, (3, 3), input_shape=image_size_for_training))\n",
    "# activation function\n",
    "model.add(Activation('relu'))\n",
    "# Aggregate data using max pooling (reduce size of feature maps)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Flatten())\n",
    "# fully connected layer with 64 output values\n",
    "model.add(Dense(64))\n",
    "# applies static function on output\n",
    "model.add(Activation('relu'))\n",
    "# randomly set 50% of all outputs to zero to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "# fully connected layer with 1 output value\n",
    "model.add(Dense(1))\n",
    "# sigmoid transformation (logistic regression) to obtain class probability\n",
    "model.add(Activation(activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the architecture. \n",
    "* You could for example get rid of all convolutional layers and only use a 'classical' neural network and see what happens.\n",
    "* Another option is to add more convolutional layers or increase/decrease the number of filters.\n",
    "* You can also play around with the _Dropout()_ layers to see whether overfitting really is a problem here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation & training\n",
    "After the model architecture has been defined, we have to compile the model. Important parameters include the optimizer and the loss function. The loss function for binary classifications is 'binary_crossentropy', which is what the model is trying to minimize during the training process. The optimizer defines how the gradients and their updates are calculated. Changing the optimizer can have a great effect on model convergence, if for example one changes to the stochastic gradient descent optimizer and chooses a high learning rate it may be that the model never converges.\n",
    "\n",
    "* https://keras.io/optimizers/\n",
    "\n",
    "#### Epochs\n",
    "Number of full passes over the training data (increase this number to get a better model performance & incrased training time).\n",
    "#### Batch Size\n",
    "Number of images simultenously used to calculated one update of the gradients. 1 image is stochastic gradient descent (SGD), N> and >1 images is mini-batch gradient descent (usually between 32 and 256 images), using all N images is gradient descent.\n",
    "#### Steps_per_epoch\n",
    "The number of batches as generated by the data generator to process per epoch. We divide the number of samples by the batch size to make one full pass over the training data per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=5,\n",
    "    workers=2,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to choose a different optimizer or to increase the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "The model may need a few epochs to learn how to distinct between different species, you can see that by observing the accuracy during training time. After only a few epochs we can take a look at a few predictions of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test sample\n",
    "\n",
    "# get a test batch\n",
    "test_batch_data = test_generator.next()\n",
    "# get class information to make correct mapping\n",
    "classes = test_generator.class_indices\n",
    "for cl, no in classes.items():\n",
    "    if no == 1:\n",
    "        class1 = cl\n",
    "# calculate predictions\n",
    "p_test = model.predict_on_batch(test_batch_data[0])\n",
    "\n",
    "# show some images and their prediction\n",
    "for i in range(0, len(test_batch_data[1])):\n",
    "    print(\"Predicted %s percent of being a %s\" %\n",
    "          (round(float(p_test[i] * 100), 2), class1))\n",
    "    plt.imshow(array_to_img(test_batch_data[0][i, :]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_train.mean\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "layer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Visualization of the filters of a model, via gradient ascent in input space.\n",
    "This script can run on CPU in a few minutes (with the TensorFlow backend).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from keras import backend as K\n",
    "\n",
    "# dimensions of the generated pictures for each filter.\n",
    "img_width = 150\n",
    "img_height = 150\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'activation_1'\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "\n",
    "kept_filters = []\n",
    "for filter_index in range(0, 32):\n",
    "    # we only scan through the first 200 filters,\n",
    "    # but there are actually 512 of them\n",
    "    print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        print('Current loss value:', loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            # break\n",
    "            pass\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value > -9999:\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 5\n",
    "\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img\n",
    "        \n",
    "plt.imshow(stitched_filters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Visualization of the filters of a model, via gradient ascent in input space.\n",
    "This script can run on CPU in a few minutes (with the TensorFlow backend).\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from keras import backend as K\n",
    "\n",
    "# dimensions of the generated pictures for each filter.\n",
    "img_width = 128\n",
    "img_height = 128\n",
    "\n",
    "# the name of the layer we want to visualize\n",
    "# (see model definition at keras/applications/vgg16.py)\n",
    "layer_name = 'conv2d_4'\n",
    "\n",
    "# util function to convert a tensor into a valid image\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "# build the VGG16 network with ImageNet weights\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "print('Model loaded.')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# this is the placeholder for the input images\n",
    "input_img = model.input\n",
    "\n",
    "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "\n",
    "kept_filters = []\n",
    "for filter_index in range(0, 200):\n",
    "    # we only scan through the first 200 filters,\n",
    "    # but there are actually 512 of them\n",
    "    print('Processing filter %d' % filter_index)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # we build a loss function that maximizes the activation\n",
    "    # of the nth filter of the layer considered\n",
    "    layer_output = layer_dict[layer_name].output\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "    else:\n",
    "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "    # we compute the gradient of the input picture wrt this loss\n",
    "    grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "    # normalization trick: we normalize the gradient\n",
    "    grads = normalize(grads)\n",
    "\n",
    "    # this function returns the loss and grads given the input picture\n",
    "    iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "    # step size for gradient ascent\n",
    "    step = 1.\n",
    "\n",
    "    # we start from a gray image with some random noise\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_img_data = np.random.random((1, 3, img_width, img_height))\n",
    "    else:\n",
    "        input_img_data = np.random.random((1, img_width, img_height, 3))\n",
    "    input_img_data = (input_img_data - 0.5) * 20 + 128\n",
    "\n",
    "    # we run gradient ascent for 20 steps\n",
    "    for i in range(20):\n",
    "        loss_value, grads_value = iterate([input_img_data])\n",
    "        input_img_data += grads_value * step\n",
    "\n",
    "        print('Current loss value:', loss_value)\n",
    "        if loss_value <= 0.:\n",
    "            # some filters get stuck to 0, we can skip them\n",
    "            break\n",
    "\n",
    "    # decode the resulting input image\n",
    "    if loss_value > 0:\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        kept_filters.append((img, loss_value))\n",
    "    end_time = time.time()\n",
    "    print('Filter %d processed in %ds' % (filter_index, end_time - start_time))\n",
    "\n",
    "# we will stich the best 64 filters on a 8 x 8 grid.\n",
    "n = 8\n",
    "\n",
    "# the filters that have the highest loss are assumed to be better-looking.\n",
    "# we will only keep the top 64 filters.\n",
    "kept_filters.sort(key=lambda x: x[1], reverse=True)\n",
    "kept_filters = kept_filters[:n * n]\n",
    "\n",
    "# build a black picture with enough space for\n",
    "# our 8 x 8 filters of size 128 x 128, with a 5px margin in between\n",
    "margin = 5\n",
    "width = n * img_width + (n - 1) * margin\n",
    "height = n * img_height + (n - 1) * margin\n",
    "stitched_filters = np.zeros((width, height, 3))\n",
    "\n",
    "# fill the picture with our saved filters\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        img, loss = kept_filters[i * n + j]\n",
    "        stitched_filters[(img_width + margin) * i: (img_width + margin) * i + img_width,\n",
    "                         (img_height + margin) * j: (img_height + margin) * j + img_height, :] = img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Networks\n",
    "Because training of a CNN requires a lot of computational power and time it is often useful to use a pre-trained CNN as a starting point. Below we download Googles Inception model, their weights from the ImageNet dataset, and we add our own last, fully-connected layer to the network and train it using our images.\n",
    "\n",
    "Another option is to use the architecture and re-train the model from scratch (without using the model weights). In this case one does not need to worry about how to define the model architecture, because finding a good architecture can take a huge amount of time due to the long evaluation process.\n",
    "\n",
    "If one only wants to re-train the last layer of a pre-trained model it is possible to calculate the features (output of the convolutional layer) only once, so called \"bottleneck files\". This greatly improves training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# load Googles inception model with imagenet weights\n",
    "base_model = InceptionV3(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=image_size_for_training)\n",
    "\n",
    "# take a look at its architecture\n",
    "base_model.summary()\n",
    "\n",
    "# set convolutional layers to non-trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# add fully connected output layer to model\n",
    "top_model = base_model.output\n",
    "top_model = Flatten()(top_model)\n",
    "top_model = Dense(256, activation='relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "top_model = Dense(1, activation='sigmoid')(top_model)\n",
    "\n",
    "# this is the model we will train (pre-trained convoltional part\n",
    "# and top layer)\n",
    "model = Model(inputs=base_model.input, outputs=top_model)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# look at model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feed data to the training pre-processing\n",
    "# WARNING: make sure the pre-processing is the same as used to train the\n",
    "# pre-trained model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=5,\n",
    "    workers=2,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.n // batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
