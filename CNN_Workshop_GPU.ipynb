{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNN) - Workshop\n",
    "\n",
    "## GPU Version: high-res rgb images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Modules and Dataset\n",
    "What is it gonna be? Cats / Dogs or Zebras / Elephants?\n",
    "Make your choice!\n",
    "* Choose a data set by setting the path\n",
    "* Choose an image size (read comments in code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from keras.preprocessing.image import array_to_img, load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Path to image data\n",
    "#path = 'D:/Studium_GD/Zooniverse/Data/cnn_workshop/PetImages/group1/'\n",
    "#path = '/host/data/cnn_workshop/EleZebra/'\n",
    "path = '/host/data/cnn_workshop/CatDog/'\n",
    "\n",
    "# get class labels\n",
    "class_labels = os.listdir(path + 'train')\n",
    "\n",
    "# image size for training, should be at least (150, 150, 3)\n",
    "# it doesn't make sense to increase size with respect to original images\n",
    "# the larger the size the more computationally expensive it is to train a CNN\n",
    "# recommended for Cat/Dog: (150, 150, 3)\n",
    "#recommended for Zebra / Elephant: (250, 250, 3)\n",
    "image_size_for_training = (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at your images\n",
    "We choose some random images and display them. It's always good to know what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at some images\n",
    "def look_at_imgs(path, n=5):\n",
    "    files = os.listdir(path)\n",
    "    # choose random images\n",
    "    random.seed(23)\n",
    "    random_ids = random.sample(population=range(0, len(files)), k=n)\n",
    "    random_imgs = [files[x] for x in random_ids]\n",
    "\n",
    "    # display\n",
    "    all_imgs = list()\n",
    "    for i in range(0, len(random_ids)):\n",
    "        img = load_img(path + random_imgs[i])\n",
    "        all_imgs.append(img)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print(path + random_imgs[i])\n",
    "    \n",
    "    return all_imgs\n",
    "\n",
    "random_imgs_0 = look_at_imgs(path + 'train/' + class_labels[0] + '/')\n",
    "random_imgs_1 = look_at_imgs(path + 'train/' + class_labels[1] + '/')\n",
    "\n",
    "# collect random images\n",
    "random_imgs = random_imgs_0\n",
    "random_imgs.extend(random_imgs_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators\n",
    "\n",
    "Keras ImageDataGenerator implements transformations, pre-processing and serving of image data during training. For example, streaming image batches from a directory while training a model. The data pre-processing & streaming is done on CPU, while the model training is done on GPU to increase efficiency (GPU version).\n",
    "\n",
    "Data augmentation is a process where images are artificially altered to create 'new'data that is similar to the original data. This helps to increase your data set size & to avoid overfitting. Data augmentation operations are: flipping, cropping & zooming.\n",
    "\n",
    "Data pre-processing operations like standardizations of pixel values have been shown to increase the efficiency of model training. Operations include: featurewise_center & featurewise_std_normalization.\n",
    "\n",
    "https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Generator\n",
    "# pre-processing of images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# batch size, the number of images returned by one call of the data generator\n",
    "# should be a number between 1 and approx. 256 (can be hardware limiting)\n",
    "batch_size = 32\n",
    "\n",
    "# generator for training data - specify pre-processing and transformations here\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# generator for test data (no data augmentation! but the same pre-processing as for the training data)\n",
    "datagen_test = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "# generator to get raw images to estimate transformation parameters like mean and std of pixel values\n",
    "datagen_raw = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    "    )\n",
    "\n",
    "raw_gen = datagen_raw.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=1000,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# get a single batch of raw images (1000 images)\n",
    "X_raw = raw_gen.next()\n",
    "\n",
    "# fit data generators on raw data to calculate transformations\n",
    "# use same batch of training data to fit train/test generators\n",
    "datagen_train.fit(X_raw[0])\n",
    "datagen_test.fit(X_raw[0])\n",
    "\n",
    "# fetch data from directory in specified batch sizes for all the generators\n",
    "train_generator = datagen_train.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "test_generator = datagen_test.flow_from_directory(\n",
    "    path + 'test/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# dummy data fetcher to look at data\n",
    "dummy_gen = datagen_train.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=5,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# lets take a look at what the data generator actually does and returns\n",
    "data_batch = dummy_gen.next()\n",
    "for i in range(0, len(data_batch[1])):\n",
    "    print(\"Label: %s\" % data_batch[1][i])\n",
    "    plt.imshow(array_to_img(data_batch[0][i, :]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not happy with the output, e.g. too crazy data augmentation, feel free to change the 'ImageDataGenerator' and run the cell again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers & Model Architecture\n",
    "Now we define a model architecture, which consists of sequential layers of operations. The 2 main blocks of a CNN are the convolutional part and the fully connected part.\n",
    "\n",
    "Convolutional layers: These layers extract features from the input (e.g. edges, corners, patterns). This block starts directly at the beginning (e.g. functions Conv2D()) and typically consists of convolution layers (Conv2d()), followed by an activation (nowadays ReLu activation), and a pooling layer (e.g. MaxPooling2D()). Multiple such constructs can be stacked.\n",
    "* https://keras.io/layers/convolutional/\n",
    "\n",
    "The fully connected part represents a classical neural network consisting of an input layer (Flatten()) which takes the features from the convolutional part, some hidden layer(s) (Dense()), and finally an output layer (Dense(), with an activation function that results in probabilities for classification tasks, like sigmoid or softmax).\n",
    "* https://keras.io/layers/core/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize some of the initialized layers applied to an image\n",
    "\n",
    "Change the Conv2D() layers, or add more layers after that and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets initialize a model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(1, # number of filter layers\n",
    "                (3,3), # x, y dimension of kernel (we're going for a 3x3 kernel)\n",
    "                input_shape=image_size_for_training))\n",
    "# Lets add a new activation layer!\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(5,5)))\n",
    "\n",
    "# predict our random images on a un-trained network\n",
    "#data_batch = dummy_gen.next()\n",
    "conv_out = model.predict(data_batch[0])\n",
    "\n",
    "# here we get rid of that added dimension and plot the image\n",
    "def visualize_img(model, img):\n",
    "    # Keras expects batches of images, so we have to add a dimension to trick it into being nice\n",
    "    img_batch = np.expand_dims(img,axis=0)\n",
    "    conv_img = model.predict(img_batch)\n",
    "    conv_img = np.squeeze(conv_img, axis=0)\n",
    "    print(conv_img.shape)\n",
    "    plt.imshow(array_to_img(conv_img))\n",
    "    plt.show()\n",
    "\n",
    "visualize_img(model, data_batch[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sequential model (layer based)\n",
    "model = Sequential()\n",
    "# Convolutional layer over 2 dimensions\n",
    "# 32 filters, each with a size of 3x3 pixels\n",
    "model.add(Conv2D(32, (3, 3), input_shape=image_size_for_training))\n",
    "# activation function\n",
    "model.add(Activation('relu'))\n",
    "# Aggregate data using max pooling (reduce size of feature maps)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Flatten())\n",
    "# fully connected layer with 64 output values\n",
    "model.add(Dense(64))\n",
    "# applies static function on output\n",
    "model.add(Activation('relu'))\n",
    "# randomly set 50% of all outputs to zero to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "# fully connected layer with 1 output value\n",
    "model.add(Dense(1))\n",
    "# sigmoid transformation (logistic regression) to obtain class probability\n",
    "model.add(Activation(activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the architecture. \n",
    "* You could for example get rid of all convolutional layers and only use a 'classical' neural network and see what happens.\n",
    "* Another option is to add more convolutional layers or increase/decrease the number of filters.\n",
    "* You can also play around with the Dropout() layers to see whether overfitting really is a problem here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation & training\n",
    "After the model architecture has been defined, we have to compile the model. Important parameters include the optimizer and the loss function. The loss function for binary classifications is 'binary_crossentropy', which is what the model is trying to minimize during the training process. The optimizer defines how the gradients and their updates are calculated. Changing the optimizer can have a great effect on model convergence, if for example one changes to the stochastic gradient descent optimizer and chooses a high learning rate it may be that the model never converges.\n",
    "\n",
    "* https://keras.io/optimizers/\n",
    "\n",
    "#### Epochs\n",
    "Number of full passes over the training data (increase this number to get a better model performance & incrased training time).\n",
    "#### Batch Size\n",
    "Number of images simultenously used to calculated one update of the gradients. 1 image is stochastic gradient descent (SGD), N> and >1 images is mini-batch gradient descent (usually between 32 and 256 images), using all N images is gradient descent.\n",
    "#### Steps_per_epoch\n",
    "The number of batches as generated by the data generator to process per epoch. We divide the number of samples by the batch size to make one full pass over the training data per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=5,\n",
    "    workers=2,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to choose a different optimizer or to increase the number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "The model may need a few epochs to learn how to distinct between different species, you can see that by observing the accuracy during training time. After only a few epochs we can take a look at a few predictions of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test sample\n",
    "\n",
    "# get a test batch\n",
    "test_batch_data = test_generator.next()\n",
    "# get class information to make correct mapping\n",
    "classes = test_generator.class_indices\n",
    "for cl, no in classes.items():\n",
    "    if no == 1:\n",
    "        class1 = cl\n",
    "# calculate predictions\n",
    "p_test = model.predict_on_batch(test_batch_data[0])\n",
    "\n",
    "# show some images and their prediction\n",
    "for i in range(0, len(test_batch_data[1])):\n",
    "    print(\"Predicted %s percent of being a %s\" %\n",
    "          (round(float(p_test[i] * 100), 2), class1))\n",
    "    plt.imshow(array_to_img(test_batch_data[0][i, :]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Networks\n",
    "Because training of a CNN requires a lot of computational power and time it is often useful to use a pre-trained CNN as a starting point. Below we download Googles Inception model, their weights from the ImageNet dataset, and we add our own last, fully-connected layer to the network and train it using our images.\n",
    "\n",
    "Another option is to use the architecture and re-train the model from scratch (without using the model weights). In this case one does not need to worry about how to define the model architecture, because finding a good architecture can take a huge amount of time due to the long evaluation process.\n",
    "\n",
    "If one only wants to re-train the last layer of a pre-trained model it is possible to calculate the features (output of the convolutional layer) only once, so called \"bottleneck files\". This greatly improves training speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# load Googles inception model with imagenet weights\n",
    "base_model = InceptionV3(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=image_size_for_training)\n",
    "\n",
    "# take a look at its architecture\n",
    "base_model.summary()\n",
    "\n",
    "# set convolutional layers to non-trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# add fully connected output layer to model\n",
    "top_model = base_model.output\n",
    "top_model = Flatten()(top_model)\n",
    "top_model = Dense(256, activation='relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "top_model = Dense(1, activation='sigmoid')(top_model)\n",
    "\n",
    "# this is the model we will train (pre-trained convoltional part\n",
    "# and top layer)\n",
    "model = Model(inputs=base_model.input, outputs=top_model)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# look at model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed data to the training pre-processing\n",
    "# WARNING: make sure the pre-processing is the same as used to train the\n",
    "# pre-trained model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=5,\n",
    "    workers=2,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.n // batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
