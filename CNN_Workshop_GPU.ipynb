{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks - Workshop\n",
    "\n",
    "## GPU Version: high-res rgb images are processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters: Choose your dataset\n",
    "What is it gonna be? Cats / Dogs or Zebras / Elephants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from keras.preprocessing.image import array_to_img, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Path to image data\n",
    "path = 'D:/Studium_GD/Zooniverse/Data/cnn_workshop/PetImages/group1/'\n",
    "# path = '/host/data/cnn_workshop/EleZebra/'\n",
    "# path = '/host/data/cnn_workshop/CatDog/'\n",
    "\n",
    "# get class labels\n",
    "class_labels = os.listdir(path + 'train')\n",
    "\n",
    "# image size for training, should be at least (150, 150, 3)\n",
    "# it doesn't make sense to increase size with respect to original images\n",
    "# the larger the size the more computationally expensive it is to train a CNN\n",
    "# recommended for Cat/Dog: (150, 150, 3)\n",
    "# recommended for Zebra / Elephant: (250, 250, 3)\n",
    "image_size_for_training = (150, 150, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at your images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Look at some images\n",
    "def look_at_imgs(path, n=5):\n",
    "    files = os.listdir(path)\n",
    "    # choose random images\n",
    "    random.seed(23)\n",
    "    random_ids = random.sample(population=range(0, len(files)), k=n)\n",
    "    random_imgs = [files[x] for x in random_ids]\n",
    "\n",
    "    # display\n",
    "    for i in range(0, len(random_ids)):\n",
    "        plt.imshow(load_img(path + random_imgs[i]))\n",
    "        plt.show()\n",
    "        print(path + random_imgs[i])\n",
    "\n",
    "look_at_imgs(path + class_labels[0] + '/')\n",
    "look_at_imgs(path + class_labels[1] + '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generators\n",
    "\n",
    "Keras ImageDataGenerator implements transformations of images and allows us to stream data from a directory while training a model. The data pre-processing & streaming is done on CPU, while the model training is done on GPU to increase efficiency.\n",
    "\n",
    "Data augmentation is a process where images are artificially altered to create 'new' artificial data that is similar. This helps to increase your data set & to avoid overfitting. Data augmentation operations are: flipping, cropping & zooming.\n",
    "\n",
    "Data pre-processing operations like standardizations of pixel values have been shown to increase the efficiency of model training. Operations include: featurewise_center & featurewise_std_normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Data Generator\n",
    "# pre-processing of images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# batch size, the number of images returned by one call of the data generator\n",
    "# should be a number between 1 and approx. 256 (can be hardware limiting)\n",
    "batch_size = 32\n",
    "\n",
    "# generator for training data\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# generator for test data (no data augmentation!)\n",
    "datagen_test = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True)\n",
    "\n",
    "# generator to get raw images to estimate transformations\n",
    "datagen_raw = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    "    )\n",
    "\n",
    "raw_gen = datagen_raw.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=1000,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# get a single batch of raw images (1000 images)\n",
    "X_raw = raw_gen.next()\n",
    "\n",
    "# fit data generators on raw data to calculate transformations\n",
    "datagen_train.fit(X_raw[0])\n",
    "datagen_test.fit(X_raw[0])\n",
    "\n",
    "# fetch data from directory in specified batch sizes\n",
    "train_generator = datagen_train.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "test_generator = datagen_test.flow_from_directory(\n",
    "    path + 'test/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# dummy generator to look at data\n",
    "dummy_gen = datagen_train.flow_from_directory(\n",
    "    path + 'train/',\n",
    "    target_size=image_size_for_training[0:2],\n",
    "    batch_size=5,\n",
    "    class_mode='binary',\n",
    "    seed=123)\n",
    "\n",
    "# lets take a look at what the data generator actually does and returns\n",
    "data_batch = dummy_gen.next()\n",
    "for i in range(0, len(data_batch[1])):\n",
    "    print(\"Label: %s\" % data_batch[1][i])\n",
    "    plt.imshow(array_to_img(data_batch[0][i, :]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "Now we define a model architecture, which consists of sequential layers of operations. The 2 blocks are the convolutional part and the fully connected part.\n",
    "\n",
    "Convolutional layers: These layers extract features from the input (e.g. edges, corners, patterns).\n",
    "Fully Connected layers: These layers take the features and assign them via weights to the output classes (standard Neural Network)\n",
    "\n",
    "Typically convolutional layers consist of a convolution, followed by an activation and a pooling layer. Multiple of such layers can be stacked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a model architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "\n",
    "# sequential model (layer based)\n",
    "model = Sequential()\n",
    "# Convolutional layer over 2 dimensions\n",
    "# 32 filters, each with a size of 3x3 pixels\n",
    "model.add(Conv2D(32, (3, 3), input_shape=image_size_for_training))\n",
    "# activation function\n",
    "model.add(Activation('relu'))\n",
    "# Aggregate data using max pooling (reduce size of feature maps)\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Flatten())\n",
    "# fully connected layer with 64 output values\n",
    "model.add(Dense(64))\n",
    "# applies static function on output\n",
    "model.add(Activation('relu'))\n",
    "# randomly set 50% of all outputs to zero to prevent overfitting\n",
    "model.add(Dropout(0.5))\n",
    "# fully connected layer with 1 output value\n",
    "model.add(Dense(1))\n",
    "# sigmoid transformation (logistic regression) to obtain class probability\n",
    "model.add(Activation(activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compilation & training\n",
    "The model is now compiled & configured. Important parameters include the optimizer and the loss function. The loss function for binary classifications is 'binary_crossentropy', which is what the model is trying to minimize during the training process. The optimizer defines how the gradients and their updates are calculated. Changing the optimizer can have a great effect on model convergence, if for example one changes to the stochastic gradient descent optimizer and chooses a high learning rate it may be that the model never converges.\n",
    "\n",
    "Increase the number of epochs (full passes over the training data) to increase training time and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# train the model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=2,\n",
    "    workers=2,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.n // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model\n",
    "We can now look at a few predictions of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict on test sample\n",
    "\n",
    "# get a test batch\n",
    "test_batch_data = test_generator.next()\n",
    "# get class information to make correct mapping\n",
    "classes = test_generator.class_indices\n",
    "for cl, no in classes.items():\n",
    "    if no == 1:\n",
    "        class1 = cl\n",
    "# calculate predictions\n",
    "p_test = model.predict_on_batch(test_batch_data[0])\n",
    "\n",
    "# show some images and their prediction\n",
    "for i in range(0, len(test_batch_data[1])):\n",
    "    print(\"Predicted %s percent of being a %s\" %\n",
    "          (round(float(p_test[i] * 100), 2), class1))\n",
    "    plt.imshow(array_to_img(test_batch_data[0][i, :]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained Networks\n",
    "Because training of a CNN requires a lot of computational power and time it is often useful to use a pre-trained CNN as a starting point. Below we download Googles Inception model, their weights from the ImageNet dataset, and we add our own last, fully-connected layer to the network and train it using our images.\n",
    "\n",
    "Another option is to use the architecture and re-train the model from scratch (without using the model weights). In this case one does not need to worry about how to define the model architecture, because finding a good architectures can take a huge amount of time due to the long evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "# load Googles inception model with imagenet weights\n",
    "base_model = InceptionV3(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=image_size_for_training)\n",
    "\n",
    "# take a look at its architecture\n",
    "base_model.summary()\n",
    "\n",
    "# set convolutional layers to non-trainable\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# add fully connected output layer to model\n",
    "top_model = base_model.output\n",
    "top_model = Flatten()(top_model)\n",
    "top_model = Dense(256, activation='relu')(top_model)\n",
    "top_model = Dropout(0.5)(top_model)\n",
    "top_model = Dense(1, activation='sigmoid')(top_model)\n",
    "\n",
    "# this is the model we will train (pre-trained convoltional part\n",
    "# and top layer)\n",
    "model = Model(inputs=base_model.input, outputs=top_model)\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# look at model architecture\n",
    "model.summary()\n",
    "\n",
    "# feed data to the training pre-processing\n",
    "# WARNING: make sure the pre-processing is the same as used to train the\n",
    "# pre-trained model\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.n // batch_size,\n",
    "    epochs=5,\n",
    "    workers=2,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.n // batch_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
